# knecht Development Rules for AI Agents

## Project Philosophy

**MANDATORY**: When user asks "What's next?", run `knecht next` FIRST (not read .knecht/tasks). Dogfooding!

**CRITICAL - NEVER BYPASS KNECHT'S INTERFACE**: 
- NEVER read `.knecht/tasks` or `.knecht/blockers` directly with `read_file`, `grep`, `cat`, or any file access tool
- ALWAYS use knecht commands: `knecht list`, `knecht show`, `knecht next`, etc.
- This applies **especially during debugging knecht itself** - that's when dogfooding matters most!
- If knecht commands don't give you what you need, that's a knecht UX bug - file a task for the missing command FIRST, then ask user what to do
- The only exception: when explicitly instructed to examine file format for a task that requires it

**knecht** is a git-native task tracker designed primarily for AI agents to work in highly structured, incremental workflows.

### Core Design Principles:

1. **Agent-First Design**: Optimize for AI agents working autonomously, not human convenience
   - Programmatic interfaces over interactive UX
   - Structured data over free-form text
   - If humans need friendly interfaces, we'll build separate tools that consume knecht's output

2. **Test-Driven Development**: Every feature starts with a failing test

3. **Self-Hosting**: We use knecht to build knecht (eating our own dog food)

4. **Pain-Driven Development**: Features are added only when their absence hurts
   - Track pain counts explicitly
   - Implement at pain count ~3-5

5. **Simplest Possible**: Sequential IDs, pipe-delimited files, no complexity

6. **YAGNI**: You Ain't Gonna Need It - defer features until they're proven necessary

7. **Structured Workflow**: Guide agents through incremental, testable work
   - Start → Test → Implement → Complete → Commit → Reflect

8. **One Task Per Session**: Complete one task, then stop. File subtasks/blockers and let user decide next steps.
9. **Extreme Programming**: Turn practices that work up to maximum (TDD, refactoring, simplicity, testing)

## Architecture Overview

### Module Structure

```
knecht/
├── src/
│   ├── main.rs      # CLI command routing and user-facing logic
│   └── task.rs      # Task struct and data operations (read/write/modify)
├── tests/
│   └── integration_test.rs  # End-to-end tests via CLI
└── .knecht/
    └── tasks        # Self-hosting: knecht's own tasks
```

**Design Decision**: Start with minimal modules. Only extract new modules when you feel pain from a file being too large or having multiple concerns.

### Data Format

File: `.knecht/tasks` - CSV format: `id,status,title,description,pain_count`

Git-friendly diffs, human-readable, sequential IDs.

## TDD Workflow - MANDATORY

### Every Feature Follows This Process:

1. **Write a Failing Test First**
   - Add test to `tests/integration_test.rs`
   - Run `cargo test` - it should FAIL
   - If test passes before you write code, the test is wrong

2. **Make It Pass with Simplest Code**
   - Write minimal code to make test pass
   - Don't add features not required by the test
   - Avoid premature abstractions

3. **Refactor If Needed**
   - Only after tests pass
   - Keep tests passing during refactoring
   - Extract functions/modules when you feel pain

4. **Track Work in knecht**
   - Use `knecht add` for new tasks
   - Use `knecht done` when complete
   - Commit `.knecht/tasks` with your changes

### Example TDD Cycle

```bash
cargo test  # Should FAIL - write test first
# Implement feature in src/
cargo test  # Should PASS
knecht done task-N
git commit -m "task-N: Description"
```

## Acceptance Criteria Guidelines

Every task requires acceptance criteria (`-a` flag). Good acceptance criteria:

**DO:**
- Be specific and testable: "cargo test passes with new test for edge case X"
- Define a clear "done" state: "Help text shows -a flag as required"
- Reference verification commands: "knecht show displays the criteria correctly"

**DON'T:**
- Be vague: "Works correctly" or "Is implemented"
- Describe the work: "Add validation to cmd_add" (that's the title, not criteria)
- Be unmeasurable: "Good user experience"

**Examples:**
- Good: "Tests pass, help updated, `knecht add` without -a shows helpful error"
- Bad: "Feature is complete and working"

## Code Style Guidelines

### Error Handling

**Current state (v0.1)**: Simple `expect()` and `unwrap()` with helpful messages

```rust
let file = fs::File::open(path).expect("Failed to open tasks file");
```

Don't prematurely add complex error handling - YAGNI! Track it as a task when you hit actual problems.

### Function Design

- Keep functions small and focused
- Prefer pure functions (input → output) when possible
- Avoid hidden global state

### Module Boundaries

**Current modules**:
- `main.rs`: CLI command parsing, user-facing output
- `task.rs`: Task struct, file I/O, data operations

**When to extract a new module**: When you feel pain from mixing concerns or file size.

## Cargo Warnings - Fix Immediately

```bash
cargo build
cargo clippy
```

**Zero tolerance for warnings**. Fix them before committing.

## Coverage and Untested Code

When running coverage checks and finding uncovered code:

1. **Delete the uncovered code first**
2. Analyze what the deleted code was doing and what behavior changed
3. Decide if the behavior change is acceptable:
   - If acceptable (dead code, unreachable defensive checks): leave it deleted
   - If unacceptable (important error handling, edge cases): write a test for it, then re-implement

Don't spend time trying to write tests for hard-to-reach code. Delete it and see what breaks in your analysis.

The goal: 100% coverage means every line has a reason to exist, proven by a test.

## Pain-Driven Feature Development

### Before Adding ANY Feature, Ask:

1. **Is there a failing test?** - No test = No feature. Period.

2. **Have I felt actual pain?**
   - "Might need" ≠ pain
   - "Would be nice" ≠ pain
   - "I can't do X and it's blocking me" = pain

3. **Am I tracking this in knecht?** - If it's not in `.knecht/tasks`, don't build it

4. **What's simpler?** - Could a shell alias or 3-line function solve this?

5. **Can I defer this?** - Almost always: YES

When pain count reaches ~3-5 instances, write a test and implement it.

## Testing Guidelines

### Integration Tests Over Unit Tests

Focus on integration tests via CLI in `tests/integration_test.rs`. Tests run the binary in temp directories. See existing tests for patterns.

### Unit Tests

Add unit tests for complex parsing logic, algorithms, or edge cases in isolated functions.

## Performance & Optimization

Don't optimize prematurely. Simple file I/O, reading entire files into memory, and linear search are all fine for v0.1.

When performance pain is real (5+ pain count), track it as a task, write a test, then optimize.

## Git & Self-Hosting

Run `git config core.hooksPath .githooks` to auto-rebuild binary after commits.

### Commit Format

`task-N: Brief description of what changed`

### Self-Hosting Workflow

```bash
knecht next          # ALWAYS first (never manually read .knecht/tasks)
knecht done task-N   # Mark complete
git add .knecht/tasks src/ tests/
git commit -m "task-N: Description"  # Confirm with user first!
```

Work on ONE task per session. When complete: mark done, reflect, prepare commit, and **end the session**. If you discover subtasks/blockers, file them and stop. Let user decide next.

## Skills (`.claude/commands/`)

Skills are markdown files that modify agent behavior. Invoke with `/skillname`.

| Skill | Purpose | When to Use |
|-------|---------|-------------|
| `/reflect` | Post-task reflection | **Required** after completing any task |
| `/upstream` | File knecht feedback from other projects | Called by /reflect when working outside knecht |
| `/journal` | Observer mode - no implementation allowed | When discussing workflows without wanting fixes |

### Skill Architecture

Skills live in `.claude/commands/*.md`. They can:
- Restrict available tools via `allowed-tools:` YAML frontmatter
- Provide structured prompts for specific workflows
- Chain to other skills (e.g., /reflect calls /upstream)

**Journal mode** (`/journal`) is special: it restricts tools so the agent can only read, search, ask questions, and file tasks - no editing or implementation. Use it for product discussions, workflow observation, or capturing insights without triggering fixes.

## Documentation

Update when:
- Adding a new command → README.md
- Changing data format → README.md and SPEC_V2_TDD.md
- Discovering architectural insights → this .rules file

Keep docs practical, current, and concise. Prefer examples over prose.

## Summary

**Always:**
- Run `knecht next` first (never read .knecht/tasks directly)
- Use knecht commands exclusively - no grep/cat on .knecht files, even during debugging
- Write failing test first, then make it pass
- Track work in knecht itself
- Fix cargo warnings immediately
- Work on one task per session, then stop
- Run `/reflect` after completing a task

**Never:**
- Read .knecht/tasks or .knecht/blockers directly (use knecht commands instead)
- Add features without tests or tracking
- Prematurely abstract or optimize
- Ignore warnings or break tests

**When in doubt:** Ask "What's simpler?" and "Can I defer this?"