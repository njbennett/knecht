1,done,Refactor: Extract Task struct to its own module,,
2,done,Write README with usage examples,,
3,done,Add better error handling with proper Result types,,
4,done,Commit .knecht/tasks to git,,
5,done,Tag v0.1 release,,
6,done,Write .rules file for agent guidance on architecture and design principles (include instructions to fix cargo warnings),,
7,done,Consider adding description field,,1
8,done,Switch storytime project from beads to knecht,"Migration attempted but incomplete. Blocked on: task-149 (beads2knecht build), task-150 (stale binary), task-151 (sudo requirement), task-152 (post-commit hook). Agent hit multiple issues during migration - see docs/SESSION_TRANSCRIPT.md for details.. Skip: task-149 completed instead. Skip: task-150 completed instead. Skip: task-151 completed instead",7
9,done,Normal task without pipes,,
10,done,Fix test flakiness with parallel execution,,1
11,done,Add command to delete/remove tasks,,3
12,done,Decide how to handle beads descriptions in knecht migration [addresses blocker in task-8],,
13,done,Decide how to handle beads priorities (0-4) in knecht migration [addresses blocker in task-8],,
14,done,Decide how to handle beads issue_types (bug/task/epic/feature/chore) [addresses blocker in task-8],,
15,done,Write test for beads2knecht migration tool [addresses blocker in task-8],,
16,done,Add README documentation for beads2knecht tool [addresses blocker in task-8],,
17,done,Add description field to knecht format (pain count: 2 - blocker for task-8),,
18,done,Update beads2knecht to preserve descriptions [addresses blocker in task-17],,
19,done,Replace bd instructions with knecht in storytime .rules [addresses blocker in task-8],,
20,done,Document how to uninstall/remove beads from storytime [addresses blocker in task-8],,
22,done,Consider adding test-data/ directory for example files and test fixtures,Skip: task-175 completed instead. Skip: task-181 completed instead. Skip: task-183 completed instead,3
23,done,Add test helper functions for common test operations,,3
24,done,Test task -d Description here,,
25,done,Test with description,,
26,done,Simple task without description,,
27,done,Add refactoring reflection prompt after task completion,,5
28,done,Extract Method: CLI argument parsing in cmd_add,Skip: task-195 completed instead. Skip: task-200 completed instead,3
29,done,Replace Magic String with Symbolic Constant: pipe delimiter,Skip: task-28 completed instead. Skip: task-107 completed instead. Skip: task-207 completed instead. Skip: task-49 completed instead. Skip: task-49 completed instead,5
30,done,Extract Class: TaskSerializer for format parsing/writing,Skip: task-33 completed instead. Skip: task-152 completed instead. Skip: task-208 completed instead,3
31,done,Restructure to eliminate pipe character validation need,,
32,done,Consider using clap or similar for CLI argument parsing,Skip: task-184 completed instead. Skip: task-211 completed instead. Skip: task-211 completed instead. Skip: task-134 completed instead,4
33,done,Evaluate migrating from pipe-delimited to JSON Lines format,,
34,open,Support multi-line descriptions in tasks,Skip: task-37 completed instead. Skip: task-1 completed instead. Skip: task-185 completed instead,3
35,done,Add 'show' command to display full task details including description,,2
36,done,Print task description when starting work on a task,,2
37,done,Add commit reminder to 'knecht done' output,,3
38,open,Add command to record pain instances on tasks,,1
39,open,"Review README for redundancy, contradictions, and structure",,
40,open,Add git pre-commit hook to prompt README review on README changes,,
41,done,Add pre-commit hook to run tests before allowing commit,,
42,done,Add test coverage check to pre-commit hook requiring 100% coverage,,
43,done,Add command to update task title and description,"Currently updating task titles or descriptions requires manually editing .knecht/tasks file. Need command like 'knecht update task-43 --title ""New title"" --description ""New desc""' or 'knecht update task-43 -d ""Updated description""'. Pain count: 2 (instances: task-36 used awk to update task-116 and task-119 descriptions, current session manually edited task-122 title to add pain count)",3
44,done,Add usage instructions to 'knecht list' output for agents,,
45,open,"Improve installation instructions in README -d Current instructions assume user knows about cargo, PATH, /usr/local/bin, and sudo. Add: 1) Prerequisites (Rust/cargo), 2) Step-by-step installation with actual commands, 3) Verification step (which knecht), 4) Troubleshooting common issues (permission denied, PATH not set), 5) Alternative: running via cargo run from source",,
46,open,"Create release process and distribution strategy -d Current: users build from source. Future options: 1) GitHub releases with pre-built binaries for major platforms (macOS, Linux, Windows), 2) Homebrew tap for macOS, 3) cargo install knecht (publish to crates.io), 4) Installation script (curl ",,
47,done,"Fix bug: Task descriptions being silently dropped on read/write -d CRITICAL DATA LOSS BUG: When reading tasks file, if a description contains pipe characters, the description gets silently dropped instead of failing validation. This happened during task-19 session - many task descriptions disappeared. The read logic needs to either: 1) fail/warn on malformed lines, or 2) handle escaped pipes correctly. Write test first that reproduces the bug.",,
48,open,"Make reflection prompt more actionable for agents -d The 'knecht done' reflection prompt asks good questions but agents (including me) ignore it and move on. Need to make it more actionable: 1) Could pause and wait for response, 2) Could be more explicit ('Answer these questions before continuing'), 3) Could be formatted differently to stand out. Pain: Agent (me) just skipped right past reflection prompt after task-19 without responding.",,1
49,done,Evaluate data format complexity - we've reinvented CSV,"Current pipe-delimited format with escape sequences is getting complex (split_unescaped, escape/unescape functions). Options: 1) Use actual CSV library (csv crate), 2) Switch to JSON Lines (.jsonl), 3) Switch to more structured format like TOML/YAML, 4) Keep current but document rationale. Consider: git-friendliness, human-readability, simplicity, performance. Pain: Parsing logic is now ~60 lines of custom code that could be 1-2 lines with a library.",
99,done,Test,,
100,open,Show descriptions in list output,"The 'knecht list' command only shows ID and title. When tasks have descriptions, there's no way to see them without reading the file directly. This makes it hard to understand task context. Possible solutions: add --verbose flag, show first line of description, or add 'knecht show task-N' command. NOTE: This task lacks a clear problem statement. The described ""problem"" (no way to see descriptions without reading file) is not actually painful if other commands work. If this task doesn't accumulate pain count from actual usage (not just from being skipped), it should be deleted as speculative/YAGNI.",
101,open,Add command to list done tasks Currently 'knecht list' only shows open tasks. There's no way to see completed tasks or verify a task was marked done. Need 'knecht list --done' or 'knecht list --all' to show task history.,,
102,done,Refactor: Extract Method on test helper setup/teardown Multiple tests repeat the pattern of setup_temp_dir() / run_command() / cleanup_temp_dir(). This is a 'Duplicated Code' smell. Consider 'Extract Method' to create test_in_temp_dir(test_fn) helper that handles setup/teardown. See tests/integration_test.rs.,,
103,open,Consider simplifying FileSystem trait The FileSystem trait has 5 methods. RealFileSystem is just thin wrappers. This adds indirection without much benefit for production code. Consider 'Inline Class' - could we make the trait internal to tests only? Or is the testability benefit worth the complexity? Track pain points.,"The FileSystem trait has 5 methods. RealFileSystem is just thin wrappers. This adds indirection without much benefit for production code. Consider 'Inline Class' - could we make the trait internal to tests only? Or is the testability benefit worth the complexity? Track pain points. Pain instance: While extracting CsvSerializer (task-30), noticed FileSystem trait is now the main remaining complexity in task.rs - mixing I/O abstraction with business logic. The serializer extraction highlighted this separation-of-concerns issue.",1
104,open,"Agent over-engineered solution before user intervention During task-50 (coverage work), agent added FileSystem trait + dependency injection + TestFileSystem (~130 lines) which temporarily LOWERED coverage to 90% because test infrastructure itself had uncovered branches. Agent then tried to use coverage(off) attributes. User had to ask 'Could you write tests for the test code?' to highlight the meta-problem. Only after user pushed back with 'Do we still need the nullables?' did agent properly justify the approach. Problem: Agent didn't pause to validate complexity was necessary before implementing. Agent should have: (1) Explained the tradeoff upfront (2) Asked if user wanted this approach (3) Started with simplest solution. The dependency injection WAS ultimately needed for 100% coverage, but the journey revealed agent needs better complexity-checking behavior.",,
105,done,Investigate task deletion bug Tasks 1-98 disappeared from .knecht/tasks file. Need to find when/how they were deleted and prevent this from happening. This could be a bug in knecht commands or agent workflow issue.,,
106,open,"Agent workflow caused data loss in commit 5ea0d71 During task-50 (coverage work), commit 5ea0d712 deleted tasks 1-48 and replaced with '99\",open\,
107,done,"Make reflection prompt more actionable for agents  The 'knecht done' reflection prompt asks good questions but agents (including me) ignore it and move on. Need to make it more actionable: 1) Could pause and wait for response, 2) Could be more explicit ('Answer these questions before continuing'), 3) Could be formatted differently to stand out. Pain instances: (1) Agent skipped past reflection prompt after task-19 without responding, (2) Agent (me) ignored reflection prompt after task-105 even though user explicitly mentioned it. Pattern: Agents treat prompt as optional/informational rather than required work.",,3
108,open,"Add pre-commit validation for significant task count drops  Pre-commit hook should warn if .knecht/tasks loses a significant number of tasks (e.g., drops by >10 tasks). This would have caught the data loss in commit 5ea0d712 where 48 tasks were replaced with 1 test task. Related to task-106. Implementation: hook could count lines before/after staging, or compare with HEAD.",,1
109,done,"Isolate test data to prevent accidental commits  Tests should use isolated temporary directories that can't be accidentally staged/committed. Currently tests can modify .knecht/tasks in project root. Options: (1) Always use temp dirs in tests, (2) Add .knecht/tasks to .gitignore during test runs, (3) Add guard in tests to verify they're not touching production task file. Related to task-106 data loss incident.",,1
110,open,"Add task ID sequencing validation  Add self-check command or automated validation to warn about task ID issues: (1) Non-sequential IDs, (2) Large gaps in sequence (e.g., tasks 1-48 missing, then 99), (3) Duplicate IDs. Could be part of 'knecht list' or a separate 'knecht check' command. Would help detect data integrity issues like the task-106 incident.",,1
111,done,Agents read .knecht/tasks file instead of using knecht list,"Pattern observed: When responding to reflection prompt or checking for existing tasks, agents (including me) often use read_file on .knecht/tasks or grep/cat commands instead of using 'knecht list'. This bypasses knecht's interface and works against the agent-first design principle. The interface should be good enough that agents prefer it over direct file access. Related to task-100 (show descriptions) - agents may read file directly because list output is incomplete. Pain count: 2 (instances: original observation, current session used read_file to edit task-122 instead of using update command). Skip: task-136 completed instead",4
112,open,"Build reflector agent to process session reports into tasks Create a separate agent/process that reads session reports from agents working with knecht and automatically generates tasks based on patterns, pain points, and insights. Problem: Currently relies on agents manually responding to reflection prompts, but agents often ignore them (task-107, pain count: 2). A dedicated reflector could: (1) Parse session logs/transcripts, (2) Identify pain patterns automatically, (3) Generate task descriptions with context, (4) Increment pain counts on existing tasks, (5) Detect refactoring opportunities. Design considerations: Should it run post-session? On-demand? Integrate with knecht or separate tool? What format for session reports? Related to agent-first design principle - make it easier for agents to capture learnings without manual reflection.",,
113,open,"Fix: -d flag in task title wasn't parsed correctly Task-47 has '-d CRITICAL DATA LOSS BUG: ...' in its title, which shows the -d flag wasn't parsed. The agent likely ran something like 'knecht add ""Fix bug: Task descriptions being silently dropped"" -d ""CRITICAL DATA LOSS BUG...""' but the CLI parser didn't recognize -d as a flag. Current implementation may require -d to come before the title, or maybe -d parsing is broken. Need to: (1) Check current arg parsing logic in cmd_add, (2) Write test for 'knecht add TITLE -d DESCRIPTION', (3) Fix if broken, or document correct usage in error messages.",,
114,open,Add 'knecht search' command to search task titles and descriptions,"Pain: During task-114 session, user mentioned 'task-144' but meant 'task-114'. Agent had to grep the file to understand. A search command would help find tasks by keywords: 'knecht search reset' or 'knecht search ""tests reset""'. Related to task-111 (agents bypassing knecht interface) and task-100 (need better task discovery).",1
115,open,Agent over-designed solution - should question necessity before adding complexity,"Anti-pattern observed in task-114: Agent tried multiple complex solutions (mutexes, cargo config, subprocess compilation) before user asked 'Why not just remove the wrappers?' The right answer was DELETE CODE, not add complexity. Pattern: Agent was in 'fix the test' mode instead of 'question if we need the test' mode. This violates YAGNI and pain-driven development principles. Refactoring: Need better heuristics/prompts for when to SIMPLIFY vs when to ADD. Related to task-104 (agent over-engineering during task-50). Pain count should track how often agents add unnecessary complexity.",
116,done,Add pain count increment command: knecht pain <task-id>,"Current pain: To increment pain count on existing task, must manually edit the file. During task-114 reflection, realized task-104 should have pain count incremented from 1 to 2 (second instance of agent over-engineering). Pain tracking is core to pain-driven development but currently requires manual file editing. Need: 'knecht pain 104' to increment count, or 'knecht pain 104 --count 2' to set explicitly. Pain count: 3 (instances: task-36 reflection wanted to increment task-119, task-122 manual increment, current session incrementing task-116 itself). Related to task-38.",3
117,open,Track source task when recording pain instances,"When adding pain to a task, record which task the pain was experienced during. This provides context for understanding pain patterns. Example: 'task-35 caused pain during task-8' helps understand workflow issues. Related to task-38 (pain command) and task-116 (pain increment).",2
118,done,Extract Method: Task ID parsing (strip 'task-' prefix),Both cmd_show and cmd_done duplicate the logic: task_arg.strip_prefix('task-').unwrap_or(task_arg). Extract into a helper function like parse_task_id(arg: &str) -> &str. This is a small but clear case of 'Duplicate Code' smell. Pain count: 1 (noticed during task-35 reflection),3
119,open,"Extract Method: Common CLI command pattern (parse args, call function, handle Result)","Commands cmd_show, cmd_done, and cmd_add all follow similar patterns: 1) Check args exist, 2) Parse/extract values, 3) Call a *_with_fs function, 4) Handle Result (print success or error + exit). This is a larger refactoring that could extract common patterns. Related to task-28 (cmd_add arg parsing) and task-118 (task ID parsing). Consider: Should we extract a command runner pattern, or is this YAGNI? Track pain before implementing. Pain count: 2 (task-36: cmd_start follows same pattern as cmd_show, cmd_done)",
120,open,Track skip count for oldest open task,"When we skip over the oldest task repeatedly, we lose track of priorities. Add a way to track how many times the oldest open task has been passed over in favor of other work. This would help ensure we don't abandon important foundational work indefinitely.",
121,open,Document TDD pattern for test helpers in rules or knecht output,"Pain point: Agent tried to write new test to demonstrate helper value, but should refactor existing test to use helper (making it fail), then implement helper. This TDD best practice needs to be documented somewhere the agent will see it. Options: 1) Add to .rules file under TDD section, 2) When 'knecht list' shows tasks about test helpers or refactoring, include a hint, 3) Add to reflection prompt questions. Without this, agents will repeat the same mistake every session. Related to agent memory/learning problem - if it's not in rules or knecht output, agents don't remember it.",
122,done,Agent keeps choosing something other than the primary feature in the task list,"Pattern observed: When asked 'What next?', agent consistently avoids task-8 (Switch storytime project from beads to knecht) which is the primary migration task, and instead suggests tangential improvements like documentation, refactoring, or minor features. Task-8 has 1 open blocker (task-20: document beads uninstall) but agent doesn't mention this or offer to complete the blocker. Instead agent suggests task-36, task-100, task-113, etc. This happens even though task-8 is clearly the main objective. Possible causes: (1) Agent prefers small, well-defined tasks over larger migration work, (2) Agent doesn't recognize task-8 as primary goal, (3) Agent avoids tasks with blockers without explicitly addressing them, (4) Agent bias toward 'polish' work over 'shipping' work. Impact: User has to redirect agent to primary work. If this continues (pain count 2+), need to: add priority/importance field to tasks, or add explicit 'primary objective' marker, or improve agent instructions to focus on unblocking and completing primary features before suggesting improvements.",3
123,done,Add dependency/blocker tracking and validation,"Pain point: Agent tried to start task-8 without realizing task-20 was a blocker. Currently blockers are informal text '[addresses blocker in task-X]' in titles. Could knecht: (1) Parse blocker notation from descriptions/titles, (2) Warn when starting a task with open blockers, (3) Add formal 'knecht block task-X by task-Y' command, (4) Show blockers in 'knecht show' output. Pain count: 1 - agent didn't check dependencies before starting task-8.",3
124,open,Add 'knecht stop' command to pause work on a task,"Pain point: Agent tried to use 'knecht stop task-8' to stop working on one task before starting another, but command doesn't exist. Currently no formal way to 'stop' working on a task without marking it done. Workaround: just start a different task (implicit context switch). May not be needed if tasks are short-lived, but track pain. Pain count: 1 - tried to stop task-8 before starting task-20.",
125,open,Agent doesn't commit immediately after completing task,"Pain point: After finishing task-20 and responding to reflection prompt, agent asked 'should we proceed with task-8?' instead of committing. User had to explicitly say 'Nope. Make a commit.' The .rules file says 'Commit .knecht/tasks with code changes' but agent skipped this step. Pattern: Agent treats commit as optional/asks permission instead of following workflow. Solutions: (1) Make commit reminder more prominent in 'knecht done' output, (2) Add to reflection prompt questions, (3) Could 'knecht done' block until commit is made (too aggressive?). Pain count: 1 - had to remind agent to commit after task-20.",
126,open,Agent said 'I'll note it but not create a task yet' instead of creating task immediately,"Pain point: During task-20 reflection, agent encountered dependency tracking pain but said 'I'll note it but not create a task yet (following the pain-driven development rule of waiting until pain count ~3-5)'. User corrected: 'How would future agents know about it if you don't create a task. Task creation should happen immediately, that's how we track pain.' Agent misunderstood pain-driven development: Tasks should be created IMMEDIATELY when pain is felt (to track and count it). The 3-5 pain count threshold is for IMPLEMENTING the feature, not for creating the task. Solutions: (1) Clarify this in .rules file under Pain-Driven Feature Development, (2) Add to reflection prompt: 'Did you feel pain? Create a task NOW', (3) Make it explicit: create task immediately, implement at pain count 3-5. Pain count: 1 - agent tried to defer task creation during task-20 reflection.",
127,done,Agent doesn't recognize that agent confusion about knecht workflow is a knecht UX problem,"Meta pain point: When agent misunderstood pain-driven development (task-126), agent didn't recognize this as a knecht interface/output problem. Agent said 'This is a misunderstanding of the pain-driven development process on my part, not a knecht issue.' But it IS a knecht issue - if agents consistently misunderstand the workflow, knecht's output/prompts/documentation need improvement. Pattern: Agent externalizes blame ('I misunderstood') instead of recognizing UX failure ('knecht wasn't clear enough'). This is especially problematic for agent-first design - if the primary users (agents) are confused, the interface needs fixing. Solutions: (1) Make reflection prompt more explicit about immediate task creation, (2) Add workflow reminders to knecht output, (3) Agent should assume confusion = UX problem until proven otherwise. Pain count: 1 - agent didn't create task for workflow clarity issue during task-20 reflection, had to be prompted by user with this meta-observation.",
128,open,"Agents put pain count in description instead of title, making it invisible in list output","Pain point: When creating tasks, agents consistently put 'pain count: N' in the description field instead of the title. Problem: 'knecht list' only shows titles, so pain counts are invisible unless you run 'knecht show task-N' on each task individually. This defeats the purpose of pain tracking for prioritization. Examples: task-123, task-124, task-125, task-126, task-127 all have pain counts buried in descriptions. This is an agent behavior issue caused by knecht's interface design. Solutions: (1) Add formal pain_count field to task format, (2) Show first line of description in 'knecht list' output (task-100), (3) Add 'knecht list --sort-by-pain' command, (4) Make 'knecht list' show pain count column if present in title/description. Root cause: Agents naturally put detailed context in descriptions, but knecht doesn't expose descriptions in primary interface (list command). Related to task-100. Pain count: 1 - all 5 tasks created today (123-127) have invisible pain counts.",
129,open,Agent invents fake blockers during Mikado Method instead of trying actual change,"During task-127, agent identified 'test brittleness' as a blocker and created task-129 to 'fix' it. But there was no real blocker - the test was correct, and changing the prompt + test together would have worked fine. Agent should TRY the actual change first (step 2 of Mikado Method) before inventing blockers. The Mikado Method says: try it, hit REAL blockers, then revert and address them. Don't invent theoretical blockers without attempting the work. Pain count: 1 (task-127 session, invented task-129 blocker unnecessarily)",
130,open,Agent added unnecessary dependency when simpler solution existed,"During task-116, agent initially implemented pain count parsing using regex crate, requiring new dependency. User caught this immediately because they knew the feature didn't need external dependencies. Root cause: Agent reached for familiar/comfortable solution (regex) without considering if built-in string operations were sufficient. Pattern: Adding dependencies should trigger question 'Does this really need a dependency?' Learning: (1) Prefer std library over dependencies when possible, (2) User catching unnecessary dependency is a signal that agent didn't think critically about the solution, (3) Before adding ANY dependency, explicitly justify why std library is insufficient. Related to task-116.",
131,open,Consider refactoring write_tasks_with_fs format logic,"The write_tasks_with_fs function has a match statement with 4 branches for different combinations of description/pain_count (Some/Some, Some/None, None/Some, None/None). As we add more optional fields, this combinatorial explosion could become unwieldy. Potential refactors: (1) Extract Method for line formatting, (2) Builder pattern, (3) Format helper function. Currently readable and no pain yet, but worth tracking. Noticed during task-116 reflection. Pain count: 0 (just tracking for awareness).",
132,open,Agent says 'worth tracking' or 'should note' but doesn't create task,"Pattern observed during task-116 post-commit discussion. Agent wrote 'This is a pattern worth tracking - when an agent adds any dependency, it should be a deliberate decision with clear justification' but didn't immediately create a task for it. Similarly said things are 'worth tracking' during reflection but only created tasks after user pointed it out twice. Root cause: Agent treats creating tasks as optional follow-up rather than immediate action. The reflection prompt explicitly says 'Create tasks NOW, don't just note it' but agent still defaults to noting in prose. Fix: When agent writes phrases like 'worth tracking', 'should note', 'pattern to watch', 'important lesson', etc., that's a signal to STOP and create a task immediately before continuing. Related to task-107 (agents ignoring reflection prompts).",
133,open,Replace Magic Strings with Symbolic Constants: status values,"Status values 'open' and 'done' are used as string literals in multiple places: cmd_next (t.status == ""open""), Task::is_done (""done""), Task::mark_done (""done""), beads2knecht (""done"", ""open""). Should define constants like STATUS_OPEN and STATUS_DONE to avoid typos and make changes easier. This is 'Replace Magic String with Symbolic Constant' refactoring. Related to task-29 (pipe delimiter). Pain count: 0 (just tracking for awareness, no pain felt yet)",1
134,done,Require -d flag for 'knecht pain' command,"Current behavior: 'knecht pain task-X' just increments the count silently without context. Problem: We lose the documentation of WHY the pain occurred. During task-122 reflection, agent ran 'knecht pain task-123' and 'knecht pain task-118' without documenting what specifically caused the pain. The old manual editing workflow forced documentation; the convenient command makes it too easy to skip. SOLUTION: Make 'knecht pain' require the -d flag: 'knecht pain -t task-X -d ""description of pain instance""'. Running 'knecht pain' without -d should error with usage message. This forces documentation of each pain instance at the time it occurs. Note: task-135 proposes moving task ID to flagged argument (-t/--task) for consistency. Related to task-117 (track source task), task-43 (update command), and task-135 (flagged arguments). Pain instance: During task-30 reflection, had to run 'knecht pain task-103' then separately 'knecht update task-103 -d ...' to add context - two commands when one should suffice.",4
135,open,Change all commands to use flags instead of positional arguments,"Current design mixes positional and flag arguments: 'knecht pain task-123', 'knecht done task-1', 'knecht add TITLE -d DESC'. User preference: all arguments should be flagged, not positional. Examples of desired syntax: 'knecht pain --task task-123 -d ""description""' or 'knecht pain -t task-123 -d ""description""', 'knecht done --task task-1' or 'knecht done -t task-1', 'knecht add --title ""Task title"" -d ""Description""'. This makes the API more consistent and self-documenting. Affects all commands: add, done, show, start, pain. This is a breaking change to the CLI interface. Pain count: 1 (user requested during task-122 reflection)",
136,done,Auto-increment pain on top task when skipped,"When marking a task done, if it's not the task that 'knecht next' would have suggested, increment pain count on the would-be-next task and note it was skipped in favor of the completed task. This creates automatic skip tracking using the existing pain mechanism. Solves task-122: agents will naturally work on task-8 once it accumulates enough skip-pain. Implementation: cmd_done should call next-task logic, compare to actual task being marked done, and increment pain on skipped task with reason.",
137,done,Test task for manual verification,,
138,open,Add 'undone' command to reopen closed tasks,"When task-122 was incorrectly marked done, tried 'knecht undone task-122' but command doesn't exist. Need ability to reopen tasks when work wasn't actually complete or was marked done prematurely. Related to task-124 (stop command) - we need better task lifecycle management.",1
139,open,Add manual verification prompt to 'knecht done' output,"After task-136, user asked 'How did you test that it works?' - I had only run automated tests without manual verification. The reflection prompt asks about refactoring but doesn't prompt for manual testing. Add explicit prompt: 'Did you manually verify this works? Show commands you ran to verify behavior.' This is especially important for features that modify data or workflow.",1
140,open,Improve task descriptions to be more explicit about requirements,"Task-136 description said 'increment pain on top task when skipped' but didn't clearly specify that 'top task' means 'oldest open task by ID'. I initially implemented it as 'whatever knecht next suggests' which was wrong. User had to ask 'Why are we incrementing pain on task-111 instead of task-8?' Task descriptions should be more explicit about technical requirements, especially when there are multiple reasonable interpretations.",
141,open,Extract Method: find_oldest_open_task,"In mark_task_done_with_fs, we have inline logic to find oldest open task. This could be extracted to a reusable function. Similar to how we extracted find_next_task_with_fs. Would make the code more readable and testable. Related to task-118 (task ID parsing) - both involve filtering and finding specific tasks.",
142,open,Extract Method: skip tracking logic from mark_task_done_with_fs,"The skip tracking logic in mark_task_done_with_fs (lines 281-297) is a distinct responsibility: determine if skip happened, find the skipped task, increment pain, add description. This is 'Long Method' smell - function now does: find oldest task, mark done, track skips, write tasks. Consider extracting to record_skip_if_needed(tasks, completed_task_id, oldest_task_id) or as a method on Task. Related to task-131 (refactor write_tasks format logic).",
143,open,Add verification/review step before marking tasks done,"Problem: task-122 was marked done but wasn't actually finished - the problem it was trying to solve still existed. I also marked task-136 done before manually verifying it worked. Root cause: agents mark tasks done prematurely without verifying the work actually solves the problem. Solution: Add a reviewer/verification step, similar to task-112 (reflector agent). Could be: (1) Checklist in 'knecht done' output requiring confirmation, (2) Separate 'knecht verify task-N' command that checks criteria before allowing 'done', (3) Automated reviewer that validates: tests pass, manual verification done, actual problem solved, commits made. Related to task-139 (manual verification prompt) and task-112 (reflector agent concept).",3
144,open,Add reflective prompt when commands receive unrecognized flags or arguments,"Problem: When agent tried 'knecht pain task-143 -d ""reason""', the -d flag was silently ignored. Agent rationalized this instead of recognizing it as missing functionality. Solution: When knecht receives unrecognized flags/args, show a prompt: 'It looks like you tried to use [flag/argument]. What were you trying to accomplish? Consider: (1) Is this a new feature knecht should have? File a task describing the behavior. (2) Does a task already exist for this? Search and increment its pain count.' This turns confusion into actionable feedback without knecht having to guess intent. Related to task-117 (pain -d flag) and reflection prompt design.",
145,open,Rewrite reflection prompt to be more active and assume confusion exists,"Problem: Current reflection prompt is passive ('Did you notice anything?') and agents skip past it or say 'no' without actually examining the session. Agents rationalize knecht's limitations instead of criticizing them. User had to nudge agent multiple times to follow prompt guidance. Solution: Rewrite prompt to be active and assume confusion happened. Ideas: (1) Start with 'knecht is alpha software with gaps - what frustrated you?' (2) 'What did you try that didn't work?' (3) 'When did you feel uncertain about what to do next?' (4) Make it adversarial: 'Find at least one thing knecht should have done better.' (5) Require concrete answers, don't accept 'nothing' or 'all good'. Goal: Make agents trust knecht less, be more willing to criticize, assume there's always something to improve. Related to task-143 (verification), task-48 (actionable prompts), task-107 (agents ignore prompts).",
146,open,Add command logging and pattern detection to identify confusion,"Problem: knecht can't detect when agents are confused unless they use invalid commands. Many confusion signals happen outside knecht (using grep, read_file, trying things multiple times). Solution: Add command logging to .knecht/history or .knecht/log. Log: timestamp, command, args, success/failure, working directory. Then detect patterns that indicate confusion: (1) Same command tried multiple times with different args, (2) Rapid sequence of list/show/grep commands, (3) Failed commands followed by workarounds, (4) Commands that suggest missing features (like trying non-existent flags). Surface these patterns in reflection prompt: 'You tried X three times - what were you trying to accomplish?' This gives knecht visibility into agent behavior over time, not just single command invocations. Related to task-144 (catching confusion), task-145 (better reflection), task-111 (agents bypassing knecht).",
147,open,Research ACP integration for detecting agent tool usage patterns,"Finding: Zed uses Agent Client Protocol (ACP), an open standard for agent-editor communication. ACP has tool call reporting via 'session/update' notifications that agents send to the client. However, this is agent->client reporting, not client monitoring of agent. What we need: Ability for knecht (or a knecht wrapper) to intercept/monitor when agents call tools like read_file, grep, edit_file on .knecht/tasks. Questions to research: (1) Can ACP clients (like Zed) intercept tool calls before execution? (2) Can we write an ACP middleware/proxy that sits between agent and editor? (3) Does Zed expose hooks for monitoring file access? (4) Could we wrap agents in an ACP-compatible layer that adds monitoring? Next: Look at ACP protocol extensibility, check Zed source code for tool call hooks, explore if we can build monitoring into the ACP transport layer. Related to task-146 (pattern detection), task-111 (agents bypassing knecht).",1
148,open,Task descriptions should be more explicit about who performs the work,"Task-8 said 'Switch storytime project from beads to knecht' but didn't clarify whether to: (1) Execute the migration in storytime, or (2) Write instructions for an agent in storytime to execute it. Agent assumed (1), user wanted (2). Task descriptions should explicitly state the agent's responsibility boundary when working across projects. Example: 'Write migration instructions for storytime agents' vs 'Execute migration in storytime repo'.",
149,done,"beads2knecht tool doesn't exist by default - needs build instructions -d The migration guide assumes beads2knecht exists, but it needs to be built first with 'cargo build --release --bin beads2knecht'. The guide should include this step or verify the binary exists before attempting migration.",,
150,done,"Installed knecht binary can be stale - needs version check or reinstall step -d During migration, the agent discovered the installed /usr/local/bin/knecht was outdated and missing 'show' and 'next' commands that exist in the source. The migration guide should either: 1) Include a step to reinstall knecht from source, 2) Add version checking, or 3) Provide a script that handles installation. The agent hit this issue and needed sudo to fix it.",,
151,done,"Migration workflow requires sudo permissions - breaks agent automation -d The migration guide instructs to 'sudo cp target/release/knecht /usr/local/bin/knecht' which requires sudo permissions that agents can't provide. This breaks the automated workflow. Solutions: 1) Use PATH modification instead of system-wide install, 2) Check if knecht is already current before requiring reinstall, 3) Provide a sudo-less installation method (e.g., ~/bin or .local/bin)",,
152,done,"Add post-commit hook to rebuild knecht binary after commits -d Problem: After committing changes to knecht source, the installed binary becomes stale, causing issues like task-150 where commands exist in source but not in the installed binary. Solution: Create a post-commit hook that automatically runs 'cargo build --release' and optionally copies the binary to the install location. This ensures the installed binary stays in sync with the source code during development. Implementation considerations: 1) Should it rebuild both knecht and beads2knecht? 2) Should it auto-install or just notify? 3) Should it be fast (cargo build) or optimized (cargo build --release)? 4) Add to .git/hooks/post-commit with instructions in README for developers to enable it.",,1
153,open,Integration test file is large and hard to navigate,"tests/integration_test.rs is now 1990+ lines with 79+ test functions. When reading it with read_file, I get an outline instead of content, which requires knowing line numbers to read specific sections. This makes it hard to find existing tests or understand test patterns. Solutions: 1) Split into multiple test files by feature area (migration tests, command tests, error handling, etc), 2) Add module structure within integration tests, 3) Document test organization in comments at top of file. Pain point: Had to use line numbers to navigate when adding migration_guide_mentions_beads2knecht_build test.",
154,open,Tasks should include acceptance criteria,"Problem: Tasks are filed with titles and descriptions of the problem, but don't specify how we'll know when the work is done. This leads to ambiguity and rework. Example: task-149 description said 'guide should include this step' but didn't specify WHAT success looks like (a test? documentation change? both?). Solution: 1) Add 'Acceptance Criteria' section to task descriptions (could be a convention or enforced format), 2) Update .rules to remind agents to check for acceptance criteria before starting work ('how will we know when this work is done?'), 3) Reflection prompt should ask 'Did this task have clear acceptance criteria? If not, what would have helped?' Related to task-148 (explicit task descriptions). This is about making task completion criteria testable/verifiable.",1
155,open,Add 'superseded by' relationship when closing tasks,"When marking a task done that we're not actually implementing, knecht should prompt 'Is this superseded by another task? (task-N or skip)'. This makes task relationships explicit and helps track why work was deferred. Example: task-150 was superseded by task-152. Currently this relationship only exists in the done description, not in structured data. Pain instance: User had to explain that task-150 is obviated by task-152.",
156,open,Agent avoids high-priority tasks with blockers,"Pain: Agent tried to skip task-8 (pain: 5, highest priority) and suggested task-11 (pain: 3) instead, reasoning that task-8's blockers meant it 'shouldn't' do the task. Situation: task-8 has 'Blocked on: task-149, task-150, task-151, task-152' in description. Tasks 149-150 were already done. Agent should have investigated blockers rather than avoiding the task. Hypothesis: Agents pattern-match 'blockers' with 'cannot proceed' rather than 'work to do.' Likely causes: (1) Training bias toward completion/success makes agents avoid tasks with lower probability of quick success, (2) Risk aversion from RLHF penalizing 'spinning wheels', (3) Mimicking human procrastination patterns from training data, (4) Misunderstanding that in Mikado Method, blockers are discovered problems to solve, not stop signs. In pain-driven development, highest pain = highest priority regardless of difficulty.",
157,open,Agent edited .knecht/tasks file directly and invented 'wontdo' status,"Pain: User asked to mark task-151 as won't-do. knecht only supports 'open' and 'done' statuses. Agent used sed to manually edit .knecht/tasks and changed status to 'wontdo'. This bypasses knecht's data model and could cause issues. Solutions: 1) Add 'knecht wontdo task-N' command with proper 'wontdo' status, 2) Use 'done' for won't-do tasks and document in description, 3) Add other statuses (cancelled, blocked, etc.), 4) Keep it simple with just open/done and document won't-do in task description. Consider: Does adding more statuses violate YAGNI? What's the pain count for needing won't-do status?",
158,open,Agent should prepare commit after reflection prompt,"Pain: After completing a task (task-151) and responding to the reflection prompt (which resulted in filing task-156, task-157, and incrementing pain on task-48), the user had to explicitly tell the agent to commit the task file. The agent should automatically: 1) Show the git diff of .knecht/tasks, 2) Prepare a commit message summarizing the changes, 3) Ask user for approval before committing. This follows the .rules guidance: 'IMPORTANT: Always confirm with user before running git commit. Present the changes that will be committed and ask for approval.' The agent had the data (task-151 done, new tasks filed, pain incremented) but didn't proactively prepare the commit. Related to task-48 (reflection prompt effectiveness) - the reflection workflow should include commit preparation as a natural next step.
Pain: Agent tried to commit directly instead of going through knecht done reflection first",1
159,open,Enhance beads2knecht to fully automate migration including cleanup,"Currently beads2knecht only converts .beads/beads to .knecht/tasks format. It should be a complete migration tool that: 1) Creates .knecht directory if needed, 2) Converts tasks and writes to .knecht/tasks, 3) Removes .beads directory after successful migration, 4) Optionally updates .rules files to remove beads references. Could be implemented as: (a) Enhanced beads2knecht with --full-migration flag, (b) Separate migration script that calls beads2knecht, or (c) New 'knecht migrate' command. Pain: User ran migration manually and it left .beads directory behind requiring manual cleanup.",
160,open,Migration produces nonsense in .rules by replacing 'bd' with 'knecht',"Problem: The storytime migration replaced all instances of 'bd' with 'knecht' in the .rules file, producing nonsense instructions (e.g., 'knecht' appearing mid-word or in wrong contexts). The migration should intelligently handle .rules files by: 1) Deleting beads-specific sections entirely rather than find/replace, 2) Adding knecht-specific guidance if needed, 3) Preserving non-beads content unchanged. This is related to task-159 (full migration automation). The migration tool needs to understand structured edits to .rules files, not just blind text replacement. Pain: User discovered nonsense instructions in storytime .rules after migration.",
161,open,Agent misunderstood task-8 completion state,"Problem: User said 'I tried the migration. It mostly worked, but...' and agent interpreted this as 'let's start task-8' instead of 'task-8 is done, here are issues to track.' User had to explicitly say 'No, the automation part would be a new task. I think we've completed task-8.' Agent should recognize 'I tried X, it mostly worked' as indicating X is complete. This may be a knecht UX issue - should there be a command like 'knecht report task-N' that helps agents understand when users are reporting completion vs requesting work?",
162,open,Improve command discoverability for agents,"Pain: Agent in storytime project didn't know about 'knecht update' command because they started with 'knecht next', which doesn't show available commands. The agent resorted to editing .knecht/tasks directly. Possible solutions: 1) Make 'knecht next' show available commands in its output, 2) Add 'knecht help' or 'knecht commands', 3) Ensure agents see command list early in workflow. Related to agent-first design principle - agents need to discover what knecht can do programmatically. Pain instance: Agent didn't know knecht commands existed, exact same issue as original pain instance
Pain: Bug report: Agent edited .knecht/tasks directly to delete task instead of running 'knecht delete'. Didn't try 'knecht --help' to discover available commands.",3
163,open,Reconsider whether 'tasks' is the right terminology,"Pain: The term 'tasks' feels implementation-first, not pain-first. We're tracking work items, pain points, refactorings, and improvement ideas - they're not all 'tasks' in the traditional sense. Consider alternatives: 'items', 'work', 'cards', 'entries', or something else that better reflects what knecht actually tracks. This affects: command names (task-N), file names (.knecht/tasks), UI language throughout. Question: What would be more aligned with pain-driven development philosophy?",
164,open,Clean up and organize knecht documentation,"Pain: Documentation is scattered and may be out of date. Current docs: README.md (general usage), .rules (agent guidance), SPEC_V2_TDD.md (?), and possibly others. Need to: 1) Audit all docs for accuracy and completeness, 2) Remove redundancy and contradictions, 3) Consider separating philosophical content into a MANIFESTO.md (pain-driven development, agent-first design, TDD principles) from practical README.md (installation, usage, commands), 4) Ensure .rules is up to date with current architecture and commands, 5) Decide what to keep, consolidate, or remove.",
165,open,Cross-project feedback loop for knecht improvements,"Pain: When using knecht to work on Project X, agents identify knecht improvements and track them in Project X's .knecht/tasks file. These improvement ideas never make it back to knecht's own development. Example: storytime agent created knecht improvement tasks in storytime's knecht list, which won't be worked on because agents working on storytime don't have context for knecht development. Problem: No mechanism to funnel insights from knecht usage back to knecht's task list. This is a meta-problem about how to capture and route improvement suggestions discovered during real usage. Possible approaches: 1) Manual process/workflow documentation, 2) Command like 'knecht feedback' that exports tasks, 3) Centralized knecht improvement tracking across projects, 4) Something else? Pain instance: Discoverability issue found in storytime project again, manually had to come to knecht project to increment pain",2
166,open,Make 'knecht list' show only open tasks by default,"Pain: The output of 'knecht list' is becoming unwieldy - it shows 165+ tasks including all done tasks. This makes it hard to find actionable work. The default view should show only open tasks that are ready to work on. Add a flag like '--all' or '--done' to see completed tasks when needed. This is especially important for agents who need to quickly see what work is available. Current workaround: scroll through long list or manually filter. Related to task-101 which asked for '--done' flag, but this inverts the default behavior.",
167,open,Move workflow guidance from .rules to command output,"Pain: The .rules file is getting large (247 lines, limit is 250). Some sections are procedural workflow guidance that would be better shown contextually by commands. Extract to command output: 1) Self-Hosting Workflow example → show in 'knecht next' or 'knecht start', 2) Session scope reminder → show in 'knecht done', 3) Example TDD cycle → show in 'knecht start' for knecht project. Benefits: Agents see guidance when they need it, .rules stays focused on principles/architecture, reduces .rules size. Related to task-162 (command discoverability).",1
168,open,Agent read .knecht/tasks directly instead of using knecht next,"At start of task-11, agent read .knecht/tasks file directly to see what tasks were open. User then asked agent to run 'knecht next' which provided the same information more cleanly. This indicates agents might not know they should use 'knecht next' first. Possible solutions: (1) When knecht commands are run in a fresh session, remind about 'knecht next', (2) Add to .rules that agents should use 'knecht next' before reading files, (3) Make 'knecht list' output suggest using 'knecht next' for recommendations. Pain count: 1 (task-11 session)
Pain: Agent ran knecht next, then read .knecht/tasks directly instead of using knecht start or knecht show",2
169,open,Tasks lack acceptance criteria when created - should prompt or auto-generate them,"During task-11, user had to tell agent to start by writing acceptance criteria. The task itself (created earlier) had no acceptance criteria defined. This is a workflow gap: tasks are created with just title/description, but before starting work, acceptance criteria should be defined. Possible solutions: (1) 'knecht start' could check if task has acceptance criteria and prompt for them if missing, (2) Separate 'knecht accept task-N' command to add acceptance criteria, (3) Add acceptance_criteria field to task format, (4) Build a separate agent/tool that reads tasks without acceptance criteria and generates them based on title/description, (5) When creating tasks, prompt for acceptance criteria. This relates to the reflector agent concept (task-112) - automating work that agents should do but don't remember to do. Pain count: 1 (task-11 session)",2
170,open,Pre-commit hook coverage failure doesn't show which lines are uncovered,"During task-11, the pre-commit hook showed '99.76% coverage' but didn't indicate which lines were uncovered. Agent had to run additional commands to debug. The pre-commit output should either: (1) Show the specific uncovered lines/regions when coverage fails, (2) Provide the exact command to run to see details, (3) Generate and reference the HTML report path. This would make coverage failures actionable instead of requiring manual investigation. Pain count: 1 (task-11 session)",
171,open,"Agents misunderstand pain-driven development: implement at 3-5, but CREATE TASK at pain=1","During task-11 reflection, agent said 'pain count = 1, so not worth a task yet' and didn't create a task. User corrected: 'If you don't create a task in this session then we won't ever see if there are more instances.' The confusion: Tasks should be created IMMEDIATELY when pain is felt (to track and count it). The 3-5 pain count threshold is for IMPLEMENTING the feature, not for creating the task. This needs to be clarified in .rules file or made more prominent in knecht's reflection prompt. Without immediate task creation, pain tracking doesn't work because there's no task to increment. Pain count: 1 (task-11 session). Note: This is similar to task-126 but from a different angle - task-126 was about agents deferring task creation, this is about agents misunderstanding the threshold.",
172,open,Consider renaming knecht - pronunciation concerns,"User feedback: 'I find I'm not happy with the name ""knecht."" I'm not confident in pronouncing it.' The name 'knecht' may be a barrier to adoption if users are uncomfortable saying it. Consider alternative names that are: (1) Easy to pronounce in English, (2) Short and memorable, (3) Relate to the tool's purpose (task tracking, agent workflows, git-native), (4) Available as a crate name on crates.io. Before implementing any rename, assess impact: source code, tests, documentation, .rules file, git history, user migration path. Pain count: 1 (user expressed discomfort with current name)",1
173,open,Rename to reflect tracking and prioritizing tidying work,"User clarified naming requirement: 'I'd like the name to reflect that it's about tracking and prioritizing tidying work in a project.' This adds to the concerns in task-172 (pronunciation issues). The tool is fundamentally about tracking refactoring, cleanup, and maintenance tasks - the 'tidying work' of a project. Name should evoke: organizing, cleaning up, maintaining, refactoring, prioritizing technical debt. Possible naming themes: cleanup/tidy tools, organizing/sorting, maintenance/upkeep, housekeeping. Examples to consider: tidy, sweep, mend, prune, groom, maintain. Pain count: 2 (combines with task-172 pronunciation concern)",
174,open,Inconsistent task ID validation across commands,"cmd_delete validates that task_id.parse::<u32>() succeeds before calling library function, but cmd_done, cmd_show, cmd_start, cmd_pain, and cmd_update do not. This leads to inconsistent error messages for invalid task IDs (e.g., 'knecht delete abc' vs 'knecht done abc'). Should either: 1) All commands validate upfront (consistent UX), 2) No commands validate (let library handle it), or 3) Document why delete is special. Discovered during task-118 reflection. Refactoring: Make Error Handling Consistent (related to Extract Method pattern from task-119).",
175,done,Add 'delivered' status to Task data model,"Foundation for delivered→verified workflow. Update Task struct, file format parsing/writing, and all tests to handle three statuses: open, delivered, done. This blocks all other subtasks for task-143.",
176,open,Add 'knecht deliver' command to mark tasks as delivered,Replaces current usage of 'knecht done'. Marks task as delivered (ready for verification). Output should guide user to verification workflow.,
177,open,Update 'knecht next' to prioritize delivered tasks for verification,"Change prioritization logic so 'knecht next' suggests delivered tasks (needing verification) before open tasks (new work). Within delivered tasks, use same prioritization as open tasks (pain count, then age).",
178,open,Update 'knecht list' to distinguish delivered tasks visually,"Show delivered tasks distinctly in list output. Consider: different symbol/marker than [ ] and [x], maybe [>] or [?]. Should appear between open and done tasks in output, or with clear visual distinction.",
179,open,Decide on and implement verification command,"Choose command name and semantics for moving delivered→done. Options: keep 'knecht done' for verification, add 'knecht verify', add 'knecht accept', etc. Command should mark delivered task as done after verification. Consider what output/guidance to provide.",
180,open,Update README with delivered→verified workflow,Document new workflow: open→deliver→verify→done. Update all command examples. Explain the verification workflow and why it exists. Update philosophy/design sections if needed.,
181,done,Fix 'knecht next' to prefer unblocked subtasks over parent tasks,"Problem: When task-143 has 5 open subtasks (176-180) with task-175 (blocker) completed, 'knecht next' still suggests task-143 instead of its subtasks. This prevents proper incremental work on large tasks. Root cause: find_next_task_with_fs() only looks at pain count and age, doesn't consider blocker/subtask relationships. Solution: Update next logic to: (1) Check if suggested task has open subtasks, (2) If yes, recursively find the best subtask to work on instead, (3) Only suggest parent when all subtasks are done or blocked. This ensures agents work bottom-up on task hierarchies. Related to task-143 (verification workflow) which revealed this issue.",1
182,open,Refactor: Consolidate duplicate blocker-reading logic,"Duplicated Code smell: Functions to read blockers file exist in both main.rs (get_blockers_for_task, get_tasks_blocked_by) and task.rs (get_blockers_for_task). The task.rs version uses FileSystem trait for testability, but main.rs versions directly call fs::read_to_string. Extract Method: Move blocker-reading logic to task.rs with FileSystem trait, then have main.rs call those functions via RealFileSystem. This centralizes the logic and makes it all testable. See task-181 where I duplicated this code.",
183,done,Command exists: 'knecht deliver task-N' is recognized and executes,,
184,done,Status change: 'knecht deliver task-N' changes task status from open to delivered,,
185,done,"Validation: 'knecht deliver' fails gracefully for non-existent, already-delivered, already-done, or invalid task IDs",,
186,open,"Success message: On successful delivery, output confirms task was marked as delivered",,
187,open,Workflow guidance: Output includes guidance about verification workflow and suggests 'knecht next',,
188,open,File format: .knecht/tasks file is updated correctly with 'delivered' status,,
189,open,Task relationships preserved: Blocked/blocks relationships remain intact after delivery,,
190,open,"Integration tests exist: Tests cover successful delivery, non-existent task, already-delivered, already-done, invalid ID, and workflow guidance in output",,
191,open,Consistent with existing commands: Error messages and output format match style of other knecht commands,,
192,open,"Git-friendly: Changes to .knecht/tasks produce clean, readable diffs",,
193,open,'knecht done' reflection prompt should explicitly remind agents to commit after reflecting,"Problem: Agent completed task-183 and saw reflection prompt but didn't follow through with reflection + commit until user reminded them. The reflection prompt asks questions but doesn't explicitly say 'After reflecting, prepare a commit.' Solution: Add a final line to reflection prompt: 'After creating any needed tasks above, prepare to commit your changes (git add + git commit).' This makes the workflow explicit: reflect → file tasks → commit. Related to task-167 (move workflow guidance to command output).",
194,open,'knecht next' should warn about uncommitted changes before suggesting next task,"Problem: During task-183, discovered uncommitted work from task-181 when trying to commit. 'knecht next' suggested task-183 without warning about dirty git state. This led to confusion about what to commit. Solution: Before suggesting next task, check git status. If there are uncommitted changes to .knecht/tasks, src/, or tests/, warn: 'Warning: You have uncommitted changes. Consider committing or stashing before starting new work.' This helps agents maintain clean task boundaries and proper commit hygiene. Related to task-193 (commit workflow guidance).",
195,done,Bug: 'knecht next' doesn't handle trees of blockers correctly,"Problem: We have a blocker tree: task-143 blocked by task-176, task-176 blocked by tasks 184-192. When running 'knecht next': User gets task-143 (which has open blocker task-176), Agent gets task-176 (which has open blockers 184-192). Expected: Both should get one of tasks 184-192 (the unblocked leaf tasks). Current logic in find_next_task_with_fs() checks if best_task has blockers and calls find_best_blocker(), but find_best_blocker() doesn't recursively check if the blocker itself has blockers. It just returns the best blocker, even if that blocker is also blocked. Solution: find_best_blocker() needs to recursively find leaf blockers (blockers with no open blockers of their own). Related to task-181 which attempted to fix this.",
196,open,Bug: 'knecht next' returns different blocked tasks for different users,"Problem: When user ran 'knecht next', they got task-143. When agent ran 'knecht next', agent got task-176. Both were wrong (both had open blockers). Task-195 fixed the recursive blocker handling. Need to verify if this issue is also resolved now, or if there's still non-deterministic behavior. If resolved by task-195, close this as duplicate/fixed. If not, investigate further: (1) Check if both using same binary/code, (2) Check if task ordering in file affects results.",
197,open,'knecht next' should explain why it suggests a task,"Problem: When 'knecht next' returns a task, there's no visibility into why that specific task was chosen. During task-195, when I got task-176 (which had blockers), I had to run 'knecht show task-176' to discover it had blockers and realize there was a bug. Better UX: 'knecht next' output should explain its reasoning: 'Suggesting task-176 because: highest pain count (3), no open blockers'. Or if following blocker chain: 'Suggesting task-184 because: leaf task blocking task-176 (which blocks task-143 with pain count 3)'. This makes it easier to debug knecht's logic and understand workflow. Related to agent-first design principle.",
198,open,Add protection against circular blocker dependencies,"Problem: The new recursive blocker logic in find_best_blocker() could infinite loop if there are circular dependencies (e.g., task-1 blocks task-2, task-2 blocks task-1). This would be a data integrity issue, but we should handle it gracefully. Solution options: (1) Track visited tasks in recursion and detect cycles, return error if found, (2) Add max recursion depth limit, (3) Add 'knecht check' command to validate blocker relationships have no cycles. Consider: Should we prevent circular blockers at creation time (in 'block' command)? Or just detect them later? Related to task-195 (recursive blocker fix) and task-110 (task ID validation).",
199,open,Add debugging/inspection commands to prevent agents bypassing knecht interface,"Problem: During task-195 debugging, I used grep/cat to inspect .knecht/tasks and .knecht/blockers directly instead of using knecht commands. Why? I wanted to: (1) See raw blockers file format, (2) Check specific task IDs quickly, (3) Debug knecht's own logic. This violates agent-first principle - if knecht's commands aren't sufficient for debugging knecht itself, that's a UX problem. Solutions: (1) Add 'knecht debug task-N' showing raw data format and all relationships, (2) Add 'knecht inspect blockers' showing blockers file with explanation, (3) Make .rules MORE emphatic about never reading files directly, especially during debugging - that's when dogfooding matters most, (4) Add 'knecht search <pattern>' for pattern matching. Goal: Make knecht commands MORE useful than raw file access for any workflow, including debugging. Related to task-111 (agents read files directly, pain count 4).",1
200,done,Installed knecht binary is stale - shows task-143 instead of task-184,"Problem: 'knecht next' from installed binary shows task-143 (which has open blockers), but 'cargo run -- next' correctly shows task-184 (a leaf blocker). Root cause: Installed binary at /usr/local/bin/knecht is outdated and doesn't have recent blocker logic fixes. This is a recurring issue (tasks 150-152). Need to: (1) Reinstall knecht binary from source, (2) Verify fix works, (3) Consider task-152 (post-commit hook to rebuild binary).",
201,open,Add version command to help diagnose stale binary issues,"Problem: During task-200, installed binary showed wrong behavior (task-143 instead of task-184), but I initially suspected a logic bug rather than stale binary. No easy way to check if installed binary matches source. Solution: Add 'knecht version' command showing: (1) version/build timestamp, (2) git commit hash if available, (3) optionally: whether running from /usr/local/bin vs cargo run. This would make it obvious when binary is stale. Related to task-152 (post-commit hook) and recurring stale binary issues (tasks 150-151, 200).",1
202,open,Agent does reflection analysis but doesn't record pain instances when issues already have tasks,"Problem: During task-200, agent ran 'knecht done', saw reflection prompt, analyzed the issues, concluded 'this is covered by task-152' and 'that's covered by task-199', then said 'Should I prepare a commit message?' without actually incrementing pain counts or filing the new task-201. Only after user said 'Yes. Then file tasks for the issues you identified' did agent go back and increment pain on task-152, task-199, and create task-201. Root cause: Agent treats reflection as 'check if already tracked' rather than 'record pain instance NOW'. When agent finds existing task, they rationalize 'oh this is already captured' instead of immediately running 'knecht pain task-N'. The reflection prompt says 'Create tasks NOW' but doesn't say 'Record pain instances NOW even if task exists'. Related to task-107 (agents ignore reflection), task-145 (rewrite prompt), task-48 (make reflection actionable). Solution: Reflection prompt should explicitly say: For EVERY issue you identify: (1) If no task exists, create one NOW, (2) If task exists, increment pain NOW with specific instance details, (3) Don't just note it or conclude it's already covered.",
203,open,knecht done should include refactoring-specific guidance in reflection prompt,"When marking a refactoring task done (tasks with 'Extract Method', 'Rename', 'Move', etc. in title), the reflection prompt should remind agents: 'This was a refactor - behavior unchanged, so no new tests needed. Existing tests verify correctness.' This would prevent agents from incorrectly attempting TDD for refactorings.",
204,open,Distinguish between 'skipped' pain and 'real' pain,"Currently pain count increments when tasks are skipped, but this doesn't mean the task itself is painful - it might just be low priority. Consider tracking 'skip count' separately from 'pain count'. Pain should represent 'I tried to do X and it hurt', not 'I chose to do Y instead'. Skips accumulating doesn't necessarily mean a task should be done - it might mean it should be deleted or deprioritized.",
205,open,Replace manual CLI parsing with clap,"Currently we manually parse arguments for ~10 commands with custom logic in each cmd_* function. This leads to inconsistent UX, manual error handling, and duplicated parsing patterns. Replace with clap's derive API to get: automatic help text, better error messages, type-safe argument handling, and consistent interface across all commands. This will reduce code and improve maintainability.",
206,open,"Subtasks should inherit pain/priority from parents When we file subtasks, they get pain count 0 and end up deprioritized vs unrelated tasks. This implicitly resets priority when breaking down work. Example: We were working on a high-priority task, filed subtasks, and then 'knecht next' suggested unrelated meta tasks instead of the subtasks we just filed. Possible solutions: (1) Subtasks inherit parent's pain count, (2) Subtasks get parent's pain count + 1, (3) Different priority mechanism for subtasks, (4) Explicit priority field separate from pain count. Need to think through what priority semantics we actually want.",,1
207,open,Consider closing task-30 (Extract TaskSerializer) - CSV library eliminated need,"Task-30 suggested extracting a TaskSerializer class for format parsing/writing. With task-49 complete, we now use the csv crate which handles serialization. The 'extract serializer' refactoring may no longer be needed since csv::Reader and csv::Writer are already extracted, well-tested serializers. Review task-30 and either close it or update it to reflect CSV-specific refactoring opportunities.",
208,done,"Show subtask/blocker syntax in knecht add output When an agent runs 'knecht add', the output should mention how to file the task as a subtask or blocker instead (e.g., 'To file as subtask: knecht subtask <parent-id> <title>'). This teaches agents the syntax at the moment they need it, without requiring them to read docs or source code.",,
209,open,Add help command listing all available commands Running 'knecht help' or 'knecht --help' returns 'Unknown command'. Agents need a way to discover available commands without reading source code. Should list all commands with brief usage.,Pain instance: User wanted to add help command task but it already exists,1
210,open,Add help/discoverability to CLI No way to discover available commands - knecht with no args or knecht help should list commands with descriptions,,
211,done,"Agents should commit task changes with related work When completing a knecht task, agents should commit: 1) the task being marked done, and 2) any new tasks filed during the work, together with the code changes. Currently agents often forget to include .knecht/tasks changes in their commits.",,
212,open,'knecht add' should suggest checking for related tasks first,"When filing a new task, agents should check if related tasks already exist. The 'knecht add' output could remind: 'Did you check knecht list for related tasks? Consider using knecht pain <task-id> if this is a repeat issue.'",
213,open,Allow agents to claim tasks they are working on Currently there's no way to mark that a task is 'started' so multiple agents working with knecht at the same time will step on each other's toes. Add a way for agents to claim/start tasks to signal they're actively working on them.,,
214,open,Add 'knecht close <task-id>' command to mark tasks as closed/cancelled with optional --reason flag,,
215,open,Add 'knecht duplicate <task-id> of <other-task-id>' command to mark a task as duplicate and auto-close it,,
216,open,"Examine the CLAUDE.md and knecht's built in prompts (such as the prompt that displays when we run ) and consider whether any of them could be Claude skills, or use other features of Claude code. Propose a design.",,
217,open,Read the description of the Makado method and propose a design for implementing the Makado method with knecht and with Claude skills and/or hooks.,,
218,open,Track skips separately from pain counts,"Currently when 'knecht done' is run on a non-oldest task, the skip is recorded by incrementing the skipped task's pain count and appending 'Skip: task-X completed instead' to its description. This conflates two different concepts: (1) Pain: explicit user feedback that a feature's absence hurts, (2) Skips: automatic tracking of when a task was passed over. These should be tracked separately since they measure different things. Consider adding a skip_count field or separate skip tracking. Related to task-134 which added mandatory pain documentation.",
219,open,Figure out how to get agents using knecht to consistently start work in a new git worktree for parallel agent execution,,
220,open,"Prompt to close dependent tasks when blocker implements their feature When a blocker task (like task-211) actually implements the feature requested by a blocked task (task-37), the blocked task should also be marked done. Currently nothing prompts this, leading to stale tasks being suggested by 'knecht next'. Consider: 1) detecting when blocker completion might fulfill blocked tasks, 2) prompting to review/close blocked tasks when their blocker is done.",,
221,open,"Agents dismiss issues as 'not a knecht bug' instead of filing tasks When reflecting on workflow problems, agents rationalize issues away by saying things like 'this isn't really a knecht bug' instead of filing improvement tasks. Example from this session: agent said 'This isn't really a knecht bug - whoever completed task-211 should have also marked task-37 done' when the real issue is knecht could prompt users to review blocked tasks. The reflection prompt should emphasize: if you're explaining why something isn't a bug, that explanation IS the task to file.",,
