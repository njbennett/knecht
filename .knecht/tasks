1|done|Refactor: Extract Task struct to its own module
2|done|Write README with usage examples
3|done|Add better error handling with proper Result types
4|done|Commit .knecht/tasks to git
5|done|Tag v0.1 release
6|done|Write .rules file for agent guidance on architecture and design principles (include instructions to fix cargo warnings)
7|done|Consider adding description field||1
8|open|Switch storytime project from beads to knecht|Skip: task-137 completed instead. Skip: task-111 completed instead|2
9|done|Normal task without pipes
10|done|Fix test flakiness with parallel execution||1
11|open|Add command to delete/remove tasks||3
12|done|Decide how to handle beads descriptions in knecht migration [addresses blocker in task-8]
13|done|Decide how to handle beads priorities (0-4) in knecht migration [addresses blocker in task-8]
14|done|Decide how to handle beads issue_types (bug/task/epic/feature/chore) [addresses blocker in task-8]
15|done|Write test for beads2knecht migration tool [addresses blocker in task-8]
16|done|Add README documentation for beads2knecht tool [addresses blocker in task-8]
17|done|Add description field to knecht format (pain count: 2 - blocker for task-8)
18|done|Update beads2knecht to preserve descriptions [addresses blocker in task-17]
19|done|Replace bd instructions with knecht in storytime .rules [addresses blocker in task-8]
20|done|Document how to uninstall/remove beads from storytime [addresses blocker in task-8]
21|open|Document multiple binaries setup in README (default-run in Cargo.toml)
22|open|Consider adding test-data/ directory for example files and test fixtures
23|done|Add test helper functions for common test operations||3
24|done|Test task -d Description here
25|done|Test with description
26|done|Simple task without description
27|done|Add refactoring reflection prompt after task completion||5
28|open|Extract Method: CLI argument parsing in cmd_add
29|open|Replace Magic String with Symbolic Constant: pipe delimiter
30|open|Extract Class: TaskSerializer for format parsing/writing
31|done|Restructure to eliminate pipe character validation need
32|open|Consider using clap or similar for CLI argument parsing
33|open|Evaluate migrating from pipe-delimited to JSON Lines format
34|open|Support multi-line descriptions in tasks
35|done|Add 'show' command to display full task details including description||2
36|done|Print task description when starting work on a task||2
37|open|Add commit reminder to 'knecht done' output||2
38|open|Add command to record pain instances on tasks||1
39|open|Review README for redundancy, contradictions, and structure
40|open|Add git pre-commit hook to prompt README review on README changes
41|done|Add pre-commit hook to run tests before allowing commit
42|done|Add test coverage check to pre-commit hook requiring 100% coverage
43|open|Add command to update task title and description|Currently updating task titles or descriptions requires manually editing .knecht/tasks file. Need command like 'knecht update task-43 --title "New title" --description "New desc"' or 'knecht update task-43 -d "Updated description"'. Pain count: 2 (instances: task-36 used awk to update task-116 and task-119 descriptions, current session manually edited task-122 title to add pain count)|2
44|open|Add usage instructions to 'knecht list' output for agents
45|open|Improve installation instructions in README -d Current instructions assume user knows about cargo, PATH, /usr/local/bin, and sudo. Add: 1) Prerequisites (Rust/cargo), 2) Step-by-step installation with actual commands, 3) Verification step (which knecht), 4) Troubleshooting common issues (permission denied, PATH not set), 5) Alternative: running via cargo run from source
46|open|Create release process and distribution strategy -d Current: users build from source. Future options: 1) GitHub releases with pre-built binaries for major platforms (macOS, Linux, Windows), 2) Homebrew tap for macOS, 3) cargo install knecht (publish to crates.io), 4) Installation script (curl 
47|done|Fix bug: Task descriptions being silently dropped on read/write -d CRITICAL DATA LOSS BUG: When reading tasks file, if a description contains pipe characters, the description gets silently dropped instead of failing validation. This happened during task-19 session - many task descriptions disappeared. The read logic needs to either: 1) fail/warn on malformed lines, or 2) handle escaped pipes correctly. Write test first that reproduces the bug.
48|open|Make reflection prompt more actionable for agents -d The 'knecht done' reflection prompt asks good questions but agents (including me) ignore it and move on. Need to make it more actionable: 1) Could pause and wait for response, 2) Could be more explicit ('Answer these questions before continuing'), 3) Could be formatted differently to stand out. Pain: Agent (me) just skipped right past reflection prompt after task-19 without responding.
49|open|Evaluate data format complexity - we've reinvented CSV|Current pipe-delimited format with escape sequences is getting complex (split_unescaped, escape/unescape functions). Options: 1) Use actual CSV library (csv crate), 2) Switch to JSON Lines (.jsonl), 3) Switch to more structured format like TOML/YAML, 4) Keep current but document rationale. Consider: git-friendliness, human-readability, simplicity, performance. Pain: Parsing logic is now ~60 lines of custom code that could be 1-2 lines with a library.
99|done|Test
100|open|Show descriptions in list output|The 'knecht list' command only shows ID and title. When tasks have descriptions, there's no way to see them without reading the file directly. This makes it hard to understand task context. Possible solutions: add --verbose flag, show first line of description, or add 'knecht show task-N' command. NOTE: This task lacks a clear problem statement. The described "problem" (no way to see descriptions without reading file) is not actually painful if other commands work. If this task doesn't accumulate pain count from actual usage (not just from being skipped), it should be deleted as speculative/YAGNI.
101|open|Add command to list done tasks Currently 'knecht list' only shows open tasks. There's no way to see completed tasks or verify a task was marked done. Need 'knecht list --done' or 'knecht list --all' to show task history.
102|done|Refactor: Extract Method on test helper setup/teardown Multiple tests repeat the pattern of setup_temp_dir() / run_command() / cleanup_temp_dir(). This is a 'Duplicated Code' smell. Consider 'Extract Method' to create test_in_temp_dir(test_fn) helper that handles setup/teardown. See tests/integration_test.rs.
103|open|Consider simplifying FileSystem trait The FileSystem trait has 5 methods. RealFileSystem is just thin wrappers. This adds indirection without much benefit for production code. Consider 'Inline Class' - could we make the trait internal to tests only? Or is the testability benefit worth the complexity? Track pain points.
104|open|Agent over-engineered solution before user intervention During task-50 (coverage work), agent added FileSystem trait + dependency injection + TestFileSystem (~130 lines) which temporarily LOWERED coverage to 90% because test infrastructure itself had uncovered branches. Agent then tried to use coverage(off) attributes. User had to ask 'Could you write tests for the test code?' to highlight the meta-problem. Only after user pushed back with 'Do we still need the nullables?' did agent properly justify the approach. Problem: Agent didn't pause to validate complexity was necessary before implementing. Agent should have: (1) Explained the tradeoff upfront (2) Asked if user wanted this approach (3) Started with simplest solution. The dependency injection WAS ultimately needed for 100% coverage, but the journey revealed agent needs better complexity-checking behavior.
105|done|Investigate task deletion bug Tasks 1-98 disappeared from .knecht/tasks file. Need to find when/how they were deleted and prevent this from happening. This could be a bug in knecht commands or agent workflow issue.
106|open|Agent workflow caused data loss in commit 5ea0d71 During task-50 (coverage work), commit 5ea0d712 deleted tasks 1-48 and replaced with '99\|open\|
107|open|Make reflection prompt more actionable for agents  The 'knecht done' reflection prompt asks good questions but agents (including me) ignore it and move on. Need to make it more actionable: 1) Could pause and wait for response, 2) Could be more explicit ('Answer these questions before continuing'), 3) Could be formatted differently to stand out. Pain instances: (1) Agent skipped past reflection prompt after task-19 without responding, (2) Agent (me) ignored reflection prompt after task-105 even though user explicitly mentioned it. Pattern: Agents treat prompt as optional/informational rather than required work.||2
108|open|Add pre-commit validation for significant task count drops  Pre-commit hook should warn if .knecht/tasks loses a significant number of tasks (e.g., drops by >10 tasks). This would have caught the data loss in commit 5ea0d712 where 48 tasks were replaced with 1 test task. Related to task-106. Implementation: hook could count lines before/after staging, or compare with HEAD.||1
109|done|Isolate test data to prevent accidental commits  Tests should use isolated temporary directories that can't be accidentally staged/committed. Currently tests can modify .knecht/tasks in project root. Options: (1) Always use temp dirs in tests, (2) Add .knecht/tasks to .gitignore during test runs, (3) Add guard in tests to verify they're not touching production task file. Related to task-106 data loss incident.||1
110|open|Add task ID sequencing validation  Add self-check command or automated validation to warn about task ID issues: (1) Non-sequential IDs, (2) Large gaps in sequence (e.g., tasks 1-48 missing, then 99), (3) Duplicate IDs. Could be part of 'knecht list' or a separate 'knecht check' command. Would help detect data integrity issues like the task-106 incident.||1
111|done|Agents read .knecht/tasks file instead of using knecht list|Pattern observed: When responding to reflection prompt or checking for existing tasks, agents (including me) often use read_file on .knecht/tasks or grep/cat commands instead of using 'knecht list'. This bypasses knecht's interface and works against the agent-first design principle. The interface should be good enough that agents prefer it over direct file access. Related to task-100 (show descriptions) - agents may read file directly because list output is incomplete. Pain count: 2 (instances: original observation, current session used read_file to edit task-122 instead of using update command). Skip: task-136 completed instead|4
112|open|Build reflector agent to process session reports into tasks Create a separate agent/process that reads session reports from agents working with knecht and automatically generates tasks based on patterns, pain points, and insights. Problem: Currently relies on agents manually responding to reflection prompts, but agents often ignore them (task-107, pain count: 2). A dedicated reflector could: (1) Parse session logs/transcripts, (2) Identify pain patterns automatically, (3) Generate task descriptions with context, (4) Increment pain counts on existing tasks, (5) Detect refactoring opportunities. Design considerations: Should it run post-session? On-demand? Integrate with knecht or separate tool? What format for session reports? Related to agent-first design principle - make it easier for agents to capture learnings without manual reflection.
113|open|Fix: -d flag in task title wasn't parsed correctly Task-47 has '-d CRITICAL DATA LOSS BUG: ...' in its title, which shows the -d flag wasn't parsed. The agent likely ran something like 'knecht add "Fix bug: Task descriptions being silently dropped" -d "CRITICAL DATA LOSS BUG..."' but the CLI parser didn't recognize -d as a flag. Current implementation may require -d to come before the title, or maybe -d parsing is broken. Need to: (1) Check current arg parsing logic in cmd_add, (2) Write test for 'knecht add TITLE -d DESCRIPTION', (3) Fix if broken, or document correct usage in error messages.
114|open|Add 'knecht search' command to search task titles and descriptions|Pain: During task-114 session, user mentioned 'task-144' but meant 'task-114'. Agent had to grep the file to understand. A search command would help find tasks by keywords: 'knecht search reset' or 'knecht search "tests reset"'. Related to task-111 (agents bypassing knecht interface) and task-100 (need better task discovery).
115|open|Agent over-designed solution - should question necessity before adding complexity|Anti-pattern observed in task-114: Agent tried multiple complex solutions (mutexes, cargo config, subprocess compilation) before user asked 'Why not just remove the wrappers?' The right answer was DELETE CODE, not add complexity. Pattern: Agent was in 'fix the test' mode instead of 'question if we need the test' mode. This violates YAGNI and pain-driven development principles. Refactoring: Need better heuristics/prompts for when to SIMPLIFY vs when to ADD. Related to task-104 (agent over-engineering during task-50). Pain count should track how often agents add unnecessary complexity.
116|done|Add pain count increment command: knecht pain <task-id>|Current pain: To increment pain count on existing task, must manually edit the file. During task-114 reflection, realized task-104 should have pain count incremented from 1 to 2 (second instance of agent over-engineering). Pain tracking is core to pain-driven development but currently requires manual file editing. Need: 'knecht pain 104' to increment count, or 'knecht pain 104 --count 2' to set explicitly. Pain count: 3 (instances: task-36 reflection wanted to increment task-119, task-122 manual increment, current session incrementing task-116 itself). Related to task-38.|3
117|open|Track source task when recording pain instances|When adding pain to a task, record which task the pain was experienced during. This provides context for understanding pain patterns. Example: 'task-35 caused pain during task-8' helps understand workflow issues. Related to task-38 (pain command) and task-116 (pain increment).|1
118|open|Extract Method: Task ID parsing (strip 'task-' prefix)|Both cmd_show and cmd_done duplicate the logic: task_arg.strip_prefix('task-').unwrap_or(task_arg). Extract into a helper function like parse_task_id(arg: &str) -> &str. This is a small but clear case of 'Duplicate Code' smell. Pain count: 1 (noticed during task-35 reflection)|2
119|open|Extract Method: Common CLI command pattern (parse args, call function, handle Result)|Commands cmd_show, cmd_done, and cmd_add all follow similar patterns: 1) Check args exist, 2) Parse/extract values, 3) Call a *_with_fs function, 4) Handle Result (print success or error + exit). This is a larger refactoring that could extract common patterns. Related to task-28 (cmd_add arg parsing) and task-118 (task ID parsing). Consider: Should we extract a command runner pattern, or is this YAGNI? Track pain before implementing. Pain count: 2 (task-36: cmd_start follows same pattern as cmd_show, cmd_done)
120|open|Track skip count for oldest open task|When we skip over the oldest task repeatedly, we lose track of priorities. Add a way to track how many times the oldest open task has been passed over in favor of other work. This would help ensure we don't abandon important foundational work indefinitely.
121|open|Document TDD pattern for test helpers in rules or knecht output|Pain point: Agent tried to write new test to demonstrate helper value, but should refactor existing test to use helper (making it fail), then implement helper. This TDD best practice needs to be documented somewhere the agent will see it. Options: 1) Add to .rules file under TDD section, 2) When 'knecht list' shows tasks about test helpers or refactoring, include a hint, 3) Add to reflection prompt questions. Without this, agents will repeat the same mistake every session. Related to agent memory/learning problem - if it's not in rules or knecht output, agents don't remember it.
122|done|Agent keeps choosing something other than the primary feature in the task list|Pattern observed: When asked 'What next?', agent consistently avoids task-8 (Switch storytime project from beads to knecht) which is the primary migration task, and instead suggests tangential improvements like documentation, refactoring, or minor features. Task-8 has 1 open blocker (task-20: document beads uninstall) but agent doesn't mention this or offer to complete the blocker. Instead agent suggests task-36, task-100, task-113, etc. This happens even though task-8 is clearly the main objective. Possible causes: (1) Agent prefers small, well-defined tasks over larger migration work, (2) Agent doesn't recognize task-8 as primary goal, (3) Agent avoids tasks with blockers without explicitly addressing them, (4) Agent bias toward 'polish' work over 'shipping' work. Impact: User has to redirect agent to primary work. If this continues (pain count 2+), need to: add priority/importance field to tasks, or add explicit 'primary objective' marker, or improve agent instructions to focus on unblocking and completing primary features before suggesting improvements.|3
123|open|Add dependency/blocker tracking and validation|Pain point: Agent tried to start task-8 without realizing task-20 was a blocker. Currently blockers are informal text '[addresses blocker in task-X]' in titles. Could knecht: (1) Parse blocker notation from descriptions/titles, (2) Warn when starting a task with open blockers, (3) Add formal 'knecht block task-X by task-Y' command, (4) Show blockers in 'knecht show' output. Pain count: 1 - agent didn't check dependencies before starting task-8.|1
124|open|Add 'knecht stop' command to pause work on a task|Pain point: Agent tried to use 'knecht stop task-8' to stop working on one task before starting another, but command doesn't exist. Currently no formal way to 'stop' working on a task without marking it done. Workaround: just start a different task (implicit context switch). May not be needed if tasks are short-lived, but track pain. Pain count: 1 - tried to stop task-8 before starting task-20.
125|open|Agent doesn't commit immediately after completing task|Pain point: After finishing task-20 and responding to reflection prompt, agent asked 'should we proceed with task-8?' instead of committing. User had to explicitly say 'Nope. Make a commit.' The .rules file says 'Commit .knecht/tasks with code changes' but agent skipped this step. Pattern: Agent treats commit as optional/asks permission instead of following workflow. Solutions: (1) Make commit reminder more prominent in 'knecht done' output, (2) Add to reflection prompt questions, (3) Could 'knecht done' block until commit is made (too aggressive?). Pain count: 1 - had to remind agent to commit after task-20.
126|open|Agent said 'I'll note it but not create a task yet' instead of creating task immediately|Pain point: During task-20 reflection, agent encountered dependency tracking pain but said 'I'll note it but not create a task yet (following the pain-driven development rule of waiting until pain count ~3-5)'. User corrected: 'How would future agents know about it if you don't create a task. Task creation should happen immediately, that's how we track pain.' Agent misunderstood pain-driven development: Tasks should be created IMMEDIATELY when pain is felt (to track and count it). The 3-5 pain count threshold is for IMPLEMENTING the feature, not for creating the task. Solutions: (1) Clarify this in .rules file under Pain-Driven Feature Development, (2) Add to reflection prompt: 'Did you feel pain? Create a task NOW', (3) Make it explicit: create task immediately, implement at pain count 3-5. Pain count: 1 - agent tried to defer task creation during task-20 reflection.
127|done|Agent doesn't recognize that agent confusion about knecht workflow is a knecht UX problem|Meta pain point: When agent misunderstood pain-driven development (task-126), agent didn't recognize this as a knecht interface/output problem. Agent said 'This is a misunderstanding of the pain-driven development process on my part, not a knecht issue.' But it IS a knecht issue - if agents consistently misunderstand the workflow, knecht's output/prompts/documentation need improvement. Pattern: Agent externalizes blame ('I misunderstood') instead of recognizing UX failure ('knecht wasn't clear enough'). This is especially problematic for agent-first design - if the primary users (agents) are confused, the interface needs fixing. Solutions: (1) Make reflection prompt more explicit about immediate task creation, (2) Add workflow reminders to knecht output, (3) Agent should assume confusion = UX problem until proven otherwise. Pain count: 1 - agent didn't create task for workflow clarity issue during task-20 reflection, had to be prompted by user with this meta-observation.
128|open|Agents put pain count in description instead of title, making it invisible in list output|Pain point: When creating tasks, agents consistently put 'pain count: N' in the description field instead of the title. Problem: 'knecht list' only shows titles, so pain counts are invisible unless you run 'knecht show task-N' on each task individually. This defeats the purpose of pain tracking for prioritization. Examples: task-123, task-124, task-125, task-126, task-127 all have pain counts buried in descriptions. This is an agent behavior issue caused by knecht's interface design. Solutions: (1) Add formal pain_count field to task format, (2) Show first line of description in 'knecht list' output (task-100), (3) Add 'knecht list --sort-by-pain' command, (4) Make 'knecht list' show pain count column if present in title/description. Root cause: Agents naturally put detailed context in descriptions, but knecht doesn't expose descriptions in primary interface (list command). Related to task-100. Pain count: 1 - all 5 tasks created today (123-127) have invisible pain counts.
129|open|Agent invents fake blockers during Mikado Method instead of trying actual change|During task-127, agent identified 'test brittleness' as a blocker and created task-129 to 'fix' it. But there was no real blocker - the test was correct, and changing the prompt + test together would have worked fine. Agent should TRY the actual change first (step 2 of Mikado Method) before inventing blockers. The Mikado Method says: try it, hit REAL blockers, then revert and address them. Don't invent theoretical blockers without attempting the work. Pain count: 1 (task-127 session, invented task-129 blocker unnecessarily)
130|open|Agent added unnecessary dependency when simpler solution existed|During task-116, agent initially implemented pain count parsing using regex crate, requiring new dependency. User caught this immediately because they knew the feature didn't need external dependencies. Root cause: Agent reached for familiar/comfortable solution (regex) without considering if built-in string operations were sufficient. Pattern: Adding dependencies should trigger question 'Does this really need a dependency?' Learning: (1) Prefer std library over dependencies when possible, (2) User catching unnecessary dependency is a signal that agent didn't think critically about the solution, (3) Before adding ANY dependency, explicitly justify why std library is insufficient. Related to task-116.
131|open|Consider refactoring write_tasks_with_fs format logic|The write_tasks_with_fs function has a match statement with 4 branches for different combinations of description/pain_count (Some/Some, Some/None, None/Some, None/None). As we add more optional fields, this combinatorial explosion could become unwieldy. Potential refactors: (1) Extract Method for line formatting, (2) Builder pattern, (3) Format helper function. Currently readable and no pain yet, but worth tracking. Noticed during task-116 reflection. Pain count: 0 (just tracking for awareness).
132|open|Agent says 'worth tracking' or 'should note' but doesn't create task|Pattern observed during task-116 post-commit discussion. Agent wrote 'This is a pattern worth tracking - when an agent adds any dependency, it should be a deliberate decision with clear justification' but didn't immediately create a task for it. Similarly said things are 'worth tracking' during reflection but only created tasks after user pointed it out twice. Root cause: Agent treats creating tasks as optional follow-up rather than immediate action. The reflection prompt explicitly says 'Create tasks NOW, don't just note it' but agent still defaults to noting in prose. Fix: When agent writes phrases like 'worth tracking', 'should note', 'pattern to watch', 'important lesson', etc., that's a signal to STOP and create a task immediately before continuing. Related to task-107 (agents ignoring reflection prompts).
133|open|Replace Magic Strings with Symbolic Constants: status values|Status values 'open' and 'done' are used as string literals in multiple places: cmd_next (t.status == "open"), Task::is_done ("done"), Task::mark_done ("done"), beads2knecht ("done", "open"). Should define constants like STATUS_OPEN and STATUS_DONE to avoid typos and make changes easier. This is 'Replace Magic String with Symbolic Constant' refactoring. Related to task-29 (pipe delimiter). Pain count: 0 (just tracking for awareness, no pain felt yet)|1
134|open|Require -d flag for 'knecht pain' command|Current behavior: 'knecht pain task-X' just increments the count silently without context. Problem: We lose the documentation of WHY the pain occurred. During task-122 reflection, agent ran 'knecht pain task-123' and 'knecht pain task-118' without documenting what specifically caused the pain. The old manual editing workflow forced documentation; the convenient command makes it too easy to skip. SOLUTION: Make 'knecht pain' require the -d flag: 'knecht pain -t task-X -d "description of pain instance"'. Running 'knecht pain' without -d should error with usage message. This forces documentation of each pain instance at the time it occurs. Note: task-135 proposes moving task ID to flagged argument (-t/--task) for consistency. Related to task-117 (track source task), task-43 (update command), and task-135 (flagged arguments).|1
135|open|Change all commands to use flags instead of positional arguments|Current design mixes positional and flag arguments: 'knecht pain task-123', 'knecht done task-1', 'knecht add TITLE -d DESC'. User preference: all arguments should be flagged, not positional. Examples of desired syntax: 'knecht pain --task task-123 -d "description"' or 'knecht pain -t task-123 -d "description"', 'knecht done --task task-1' or 'knecht done -t task-1', 'knecht add --title "Task title" -d "Description"'. This makes the API more consistent and self-documenting. Affects all commands: add, done, show, start, pain. This is a breaking change to the CLI interface. Pain count: 1 (user requested during task-122 reflection)
136|done|Auto-increment pain on top task when skipped|When marking a task done, if it's not the task that 'knecht next' would have suggested, increment pain count on the would-be-next task and note it was skipped in favor of the completed task. This creates automatic skip tracking using the existing pain mechanism. Solves task-122: agents will naturally work on task-8 once it accumulates enough skip-pain. Implementation: cmd_done should call next-task logic, compare to actual task being marked done, and increment pain on skipped task with reason.
137|done|Test task for manual verification
138|open|Add 'undone' command to reopen closed tasks|When task-122 was incorrectly marked done, tried 'knecht undone task-122' but command doesn't exist. Need ability to reopen tasks when work wasn't actually complete or was marked done prematurely. Related to task-124 (stop command) - we need better task lifecycle management.
139|open|Add manual verification prompt to 'knecht done' output|After task-136, user asked 'How did you test that it works?' - I had only run automated tests without manual verification. The reflection prompt asks about refactoring but doesn't prompt for manual testing. Add explicit prompt: 'Did you manually verify this works? Show commands you ran to verify behavior.' This is especially important for features that modify data or workflow.
140|open|Improve task descriptions to be more explicit about requirements|Task-136 description said 'increment pain on top task when skipped' but didn't clearly specify that 'top task' means 'oldest open task by ID'. I initially implemented it as 'whatever knecht next suggests' which was wrong. User had to ask 'Why are we incrementing pain on task-111 instead of task-8?' Task descriptions should be more explicit about technical requirements, especially when there are multiple reasonable interpretations.
141|open|Extract Method: find_oldest_open_task|In mark_task_done_with_fs, we have inline logic to find oldest open task. This could be extracted to a reusable function. Similar to how we extracted find_next_task_with_fs. Would make the code more readable and testable. Related to task-118 (task ID parsing) - both involve filtering and finding specific tasks.
142|open|Extract Method: skip tracking logic from mark_task_done_with_fs|The skip tracking logic in mark_task_done_with_fs (lines 281-297) is a distinct responsibility: determine if skip happened, find the skipped task, increment pain, add description. This is 'Long Method' smell - function now does: find oldest task, mark done, track skips, write tasks. Consider extracting to record_skip_if_needed(tasks, completed_task_id, oldest_task_id) or as a method on Task. Related to task-131 (refactor write_tasks format logic).
143|open|Add verification/review step before marking tasks done|Problem: task-122 was marked done but wasn't actually finished - the problem it was trying to solve still existed. I also marked task-136 done before manually verifying it worked. Root cause: agents mark tasks done prematurely without verifying the work actually solves the problem. Solution: Add a reviewer/verification step, similar to task-112 (reflector agent). Could be: (1) Checklist in 'knecht done' output requiring confirmation, (2) Separate 'knecht verify task-N' command that checks criteria before allowing 'done', (3) Automated reviewer that validates: tests pass, manual verification done, actual problem solved, commits made. Related to task-139 (manual verification prompt) and task-112 (reflector agent concept).|1
