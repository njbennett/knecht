99|done|Test
100|open|Show descriptions in list output The 'knecht list' command only shows ID and title. When tasks have descriptions, there's no way to see them without reading the file directly. This makes it hard to understand task context. Possible solutions: add --verbose flag, show first line of description, or add 'knecht show task-N' command.
101|open|Add command to list done tasks Currently 'knecht list' only shows open tasks. There's no way to see completed tasks or verify a task was marked done. Need 'knecht list --done' or 'knecht list --all' to show task history.
102|open|Refactor: Extract Method on test helper setup/teardown Multiple tests repeat the pattern of setup_temp_dir() / run_command() / cleanup_temp_dir(). This is a 'Duplicated Code' smell. Consider 'Extract Method' to create test_in_temp_dir(test_fn) helper that handles setup/teardown. See tests/integration_test.rs.
103|open|Consider simplifying FileSystem trait The FileSystem trait has 5 methods. RealFileSystem is just thin wrappers. This adds indirection without much benefit for production code. Consider 'Inline Class' - could we make the trait internal to tests only? Or is the testability benefit worth the complexity? Track pain points.
104|open|Agent over-engineered solution before user intervention During task-50 (coverage work), agent added FileSystem trait + dependency injection + TestFileSystem (~130 lines) which temporarily LOWERED coverage to 90% because test infrastructure itself had uncovered branches. Agent then tried to use coverage(off) attributes. User had to ask 'Could you write tests for the test code?' to highlight the meta-problem. Only after user pushed back with 'Do we still need the nullables?' did agent properly justify the approach. Problem: Agent didn't pause to validate complexity was necessary before implementing. Agent should have: (1) Explained the tradeoff upfront (2) Asked if user wanted this approach (3) Started with simplest solution. The dependency injection WAS ultimately needed for 100% coverage, but the journey revealed agent needs better complexity-checking behavior.
105|done|Investigate task deletion bug Tasks 1-98 disappeared from .knecht/tasks file. Need to find when/how they were deleted and prevent this from happening. This could be a bug in knecht commands or agent workflow issue.
106|open|Agent workflow caused data loss in commit 5ea0d71 During task-50 (coverage work), commit 5ea0d712 deleted tasks 1-48 and replaced with '99\|open\|Test'. Root cause analysis: Agent likely created test data in .knecht/tasks during testing, then accidentally staged and committed test file instead of preserving real tasks. The commit message makes no mention of tasks file changes, confirming it was unintentional. Evidence: (1) Commit adds extensive test infrastructure with TestFileSystem, (2) '99\|open\|Test' looks like test data not real task, (3) Tests create/modify task files. Impact: Lost 48 tasks but recoverable from git history (commit bb72676). Prevention ideas if this happens again: (1) Pre-commit hook to warn if task count drops significantly, (2) Agent checklist before committing to review staged changes to .knecht/tasks, (3) Keep test data in tests/fixtures/ not .knecht/, (4) Add .knecht/tasks to gitignore during test runs. Track as pain point - if happens again (pain count: 2+), implement prevention.
107|open|Make reflection prompt more actionable for agents (pain count: 2) The 'knecht done' reflection prompt asks good questions but agents (including me) ignore it and move on. Need to make it more actionable: 1) Could pause and wait for response, 2) Could be more explicit ('Answer these questions before continuing'), 3) Could be formatted differently to stand out. Pain instances: (1) Agent skipped past reflection prompt after task-19 without responding, (2) Agent (me) ignored reflection prompt after task-105 even though user explicitly mentioned it. Pattern: Agents treat prompt as optional/informational rather than required work.
108|open|Add pre-commit validation for significant task count drops (pain count: 1) Pre-commit hook should warn if .knecht/tasks loses a significant number of tasks (e.g., drops by >10 tasks). This would have caught the data loss in commit 5ea0d712 where 48 tasks were replaced with 1 test task. Related to task-106. Implementation: hook could count lines before/after staging, or compare with HEAD.
109|open|Isolate test data to prevent accidental commits (pain count: 1) Tests should use isolated temporary directories that can't be accidentally staged/committed. Currently tests can modify .knecht/tasks in project root. Options: (1) Always use temp dirs in tests, (2) Add .knecht/tasks to .gitignore during test runs, (3) Add guard in tests to verify they're not touching production task file. Related to task-106 data loss incident.
110|open|Add task ID sequencing validation (pain count: 1) Add self-check command or automated validation to warn about task ID issues: (1) Non-sequential IDs, (2) Large gaps in sequence (e.g., tasks 1-48 missing, then 99), (3) Duplicate IDs. Could be part of 'knecht list' or a separate 'knecht check' command. Would help detect data integrity issues like the task-106 incident.
111|open|Agents read .knecht/tasks file instead of using knecht list (pain count: 1) Pattern observed: When responding to reflection prompt or checking for existing tasks, agents (including me) often use read_file on .knecht/tasks or grep/cat commands instead of using 'knecht list'. This bypasses knecht's interface and works against the agent-first design principle. The interface should be good enough that agents prefer it over direct file access. Related to task-100 (show descriptions) - agents may read file directly because list output is incomplete. Track pain: if happens repeatedly, need to improve 'knecht list' output to be more useful than raw file.
112|open|Build reflector agent to process session reports into tasks Create a separate agent/process that reads session reports from agents working with knecht and automatically generates tasks based on patterns, pain points, and insights. Problem: Currently relies on agents manually responding to reflection prompts, but agents often ignore them (task-107, pain count: 2). A dedicated reflector could: (1) Parse session logs/transcripts, (2) Identify pain patterns automatically, (3) Generate task descriptions with context, (4) Increment pain counts on existing tasks, (5) Detect refactoring opportunities. Design considerations: Should it run post-session? On-demand? Integrate with knecht or separate tool? What format for session reports? Related to agent-first design principle - make it easier for agents to capture learnings without manual reflection.
